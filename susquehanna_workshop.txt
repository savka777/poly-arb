denying your system in such a way that it satisfies the requirements, when it comes to speed, food push, and reliability. Some of these criteria are naturally opposing, such as speed versus reliability. Reliability by its nature includes checks and fiance, which often slow processing. For example, suppose you were tasked with designing your car to travel to your desert. You might include VRCs, such as an extra large few time, or spare me. If, on the other hand, you were tasked with designing your car to raise the formula one. You would fulfil these days, because they would slowly hide out. Bosh, when you want both speed and reliability, like an an alternated training system, you either need to make trade-offs or come up with a more complicated solution, that can satisfy both. So how important is speed when it comes to trading? These diagram here shows reaction time data from the Yorks exchange. By reaction time here, I mean, the time between when a trade, there is about information that might make them on to trade, and the trade actually happening. As you can see here, we're dealing in the order of nanoscience. So if you're not in nanoseconds, you're really not the racist when it concentrating. Both that speed cannot comp at the expense of our longer liability. If we miss even one pocket of information, our view of the market is compromised. Therefore, building trading systems, we are forced to deal with solutions that deliver a group of speed and reliability. Our choice of networking protocol is one example where we need to consider the tradeoffs between these two. In college, you may have learned that UDP is a good choice when you want to prioritise speech. On the other hand, TCP is a better choice of protocol when you want to prioritise reliability. So, what do each choose in between each book? Well, typically the exchange will transformation information using UGP. But they need to make some modifications to their system so that they not compromise on reliability. This is a simplified example of how the exchange works, putting access. The exchange is streaming packets of information over UDP on an incremental feed. There are 2 fees which are identical and A and B feet. It is therefore on the client to detect if there is a pocket, by comparison between the 2 fees. They also provide a snapshot to save us for a client who is recovering for stacking all from today. So let's start a little bit deeper into the how this snapshotting service works. So this diagram attempts to depict the snapshotting. So we can see the blue loops indicate the snapshot. So this snapshot tells you what the state of the market is at the current point in time. So, for example, the exchange might have submitted a snapshot, and they'll say, this is the current state of the market, I would say, 10 a.m. This isn't instantaneous, it takes some time to send all this information, and then once in a sound exchange, we'll send another snapshot. I think this is the state of the records past 10 or 5 AM. So, in this diagram, here you can see the 1st blue symbol indicates a snapshot, let's say that encapsulated the data in all the green parts of the section. Now, while that snapshot is being sent, there is still marketing, a live market data being transmitted, represented by the pig. So, for a point, trying to process the Snapchat, is almost like trying to concentrate on already left the station. You're trying to process the snapshot, but at the same time, you're trying to process the wide market data. So by the time you fully process the snapshot, you then need to update your view with the live data that's happening in between. Ooh. So this is just one example of how we need to balance speed versus reliability. Now, these are three, these are three are attributes, we have speed reliability and simplicity, and often we're designing our tools and our applications, we need to prioritise two of the costs of the period. It's quite hard to get all three speed simplicity on the liability, um in an application. So let's consider these three examples. A block testing tool for studying hypothesis. Anyone have any suggestions of what 2 might be prioritised here? What? Liability? Yeah, the liability would be important. I suppose when we're considering what I've done here. So if we have a block testing too, a block testing too, and maybe if it started running, it's not running live in the exchange, it's not sending orders. So if it was to crash, we restarted it, that might be the end of the world. So what my liability is a point, it's probably what we might compromise in this scenario. We'd want to see, we don't want to back taste it, back testing pipe lens one all day. And we want simplistic. We want, you know, their their concert, usually, usually using deeply normal books or something, which is, what about mechanism for being 1st to market with a border, not a trade opportunity. That really important. What would we definitely want to call are definitely going to prioritise here? You know, speed and probably reliability, because we don't want a mistake. Exactly. Exactly. So speed is definitely going to be important here. Your liability is going to be important. So, chances are, we're gonna have to compromise on info and simplicity. This is going to be a complex system. What about a mechanism for a manual intervention to shut down wall training? So, something that's gone wrong and we want to shut down all trading. What are we going to want to hear? To be simple, and also very fast, because he wants to be able to impact it and then shut down very fast. Okay, so definitely, we want it to be simple, we want to, we want to do this, we want to go over the computer, study over it. But we also need us to be realised, if we want to shut down all training, We're going to wanna make sure that it actually shuts it down. So we probably prioritise simplicity and reliability in this scenario. Okay, let's look at another example, which is creating on the exchange. So this is a relatively simple example, we have a processor, an application, running in Dublin, and it's trans, we are attending recommendation, American Trades, we want to make to an exchange, let's say. We have one of our traders sitting at their desk in Dublin, and they have a UI that allows them to interact with the market. So when they want to make a change, they're not really press a button, and they send the portage to the exchange. This is relatively simple. The trainer knows exactly when they have made it, wherever they have sent him order, and in fact, we won't have order, it was four. But it's not super fast. It means that trader needs to process the market information, they need to think about what trade they want to make, and then they need to manually impress the market. So we could speed this off. We could speed this off by having an alternated training algorithm that is automatically consuming the market information. It's processing it, designing the mortgage to send, and then automatically send before it is to the exchange. While this is a lot faster, it's not quite a simple, it can be a lot more complex. Sometimes the trader might ask us, why was this word, except, we might need to run back testing, or look at logs to figure out exactly what information the Australian had at that time, and why it made the decision how it did. But we can actually even do better than this. The information needs to be sent from our service in Dublin all the way to the exchange, which could be halfway across the continent. So, how are we going to do better? We can move our algorithm. that was running in Dublin to the exchange. We always call this colon, our colocation, because our application is running colocations with the exchange. This will really speed up, but it's quite complex. Our server is running in a different location, so we definitely wanted to be reliable. Okay, we also might want to talk about the two of us. So choosing the white tool for the job. So when we're studying the applications that the traders use and for indumbent so that they can take their view of the micro quiz, We'd often meet by this COVID and CChat. This allows us to have good UIs. When we're writing the software algorithm, that would be running in an exchange, for example, in London, we'd also do sea shark, or sometimes C++, when we want to prioritise feet, a little bit more. We often have custom hybrid, such as FPGAs, running in the exchange, and we've used custom software for that, things like very likely to use each year. And then if we have quantum or money's back studies, they might use python, do their notebooks for processing their data. Okay, so now that we have talked about the data processing pipeline and the things we might want to consider, let's look at how we can optimise our pipeline. For this, we're going to talk through any particular example that we enchanted in Salcaton recently with one of our data-driven training applications. So this data driven changes, training application is what we call a modelling application. So bringing this down very simply. We're just trying to compute a true price, or sometimes called a fare, or theo, for financial instrument. Once we have this, the trading decision becomes very easy. We final, and so final, essentially. Suicide. So, the problem we encountered with this application was that it was initially designed to process about 10 stops. It would take in information, it would calculate the fair price on this, and then it would publish information. Some of that will be published to your automated training strategy, and then the information was also published to a UI, which allowed the traders and the quads to view what was happening in the model and to tweak their primaries. And the problem we included, that over time, this application required that we decided to process single and more stocks, sold to Â£3000 stocks. The application became very slow and lagging, which was a problem. So we needed to optimise this. So, as we move right optimising an application, such as this. So the first thing we need to do is we need to figure out exactly what the process is doing. And we also need to figure out how much time we're spending in each area off the shows. So the first stop was to identify the source of the log. Profiling an application and a scenario like this isn't valuable to figure out exactly where the mountain X are. So, this is a flame graph of our application that was generated using high spike. In it, we can see that there are two main areas that are causing, but that are taking off most of the processing time. So the 1st is where the true price is population, but actually being dumped. And the 2nd was the broadcasting of information. So the publishing of information from this application. So, let's take some time to talk through each of these 2 bottlenecks and how we might try to sleep them off. So optimising the true price calculation. We looked through the different steps that this calculation was doing. So realistically, we couldn't take any of these steps, they were all essential through the process. So we found that we could introduce cushing. That was a small quick win. We shaved off by 5% from each iteration. Next, we look at hermalic scheduling. So we break down this process into kind of 3 soft processes. So in note A, it takes in the data, piruses, and normalises it. It then sends the data to no V, where the tree price is computed, and then it sends the data on to notice. C, where sanity checks and validation is reported. Each of these nodes runs on a tightly attired. So harmonic scheduling is the process of fine tuning these timers to make sure that the data flows seamlessly to the application so that there's no build-up of work and that there's no load waiting for work instead of your CVT. So by tweaking or brownies, we were able to make the data flow more seamlessly, and this speed up, sped up each iteration, we provide about 10 seconds. Okay, so that was how we handle the 2 price calculation price. The older boss calls, like, within the publishing of data. So the initial implementation here was that we would send all data as raw adjacent over web socket. Anyone have any ideas how we might go by trying to optimise this? Using a more optimised format. Okay, cool. Yeah, that's definitely good. Jason isn't the most... Yes. Yeah, PC. Something like GRPC? Okay, look, looking at the protocol that might be better. Thank you. Yep, definitely. And the shop web socket, looking, like, server side trending, and making it to request rather than two side connection, and expecting both to run and wait for that. Exactly. So, Jason and Web stock, really, really into pastors. They worked for it. It was a small application, but lots of scales, they weren't very feasible. So what the meeting? The first thing we did was we tried to look at the data in the payload. So what data will we actually send? So, as I said, before, we were sending this data to an motivated training strategy, in reality, this only needed the true price calculation. It didn't need all the information. The purpose of sending all the information was to send it to a UI, so the traders and fonts could monitor the application, and could treat their parameters. So we went to, we talked to the traders, and we discussed with them, what the work normally looked like, where all these problems with all this data are really necessary. After analysing their workflow and talking with them, we realised that a lot of the data was really redundant, and that we could remove about 80% of the data in the payload. So this gave us huge benefits. We reduced the payroll for about 120 megabytes to 36 megabytes. Next, we looked at data encoding. So in the initial implementation, we were sending over all data on every iteration. Delta encoding of the process by which you just send over the values that have changed. So, um, let's use an example to illustrate this. I'd ask you to imagine in your head an animated image of a girl with a smiling face and dark hair and an orange headband. I'd imagine you picture something like this. Now, if I asked you to imagine the same image, but this time the girl has a green headband. I'd imagine you picture something like this. This is an example that illustrates delta, including in the I have given you all of the relevant information about the 2nd image, by just telling you what has changed from the 1st image. So by applying down to encoding to our process, when we just send over the information that has changed, this also fairly reduced the payload. Like, this is great, but I'll just come on if you like to challenge yourselves. So we wanted to go through this, again, even though we greatly reduce the payload. So we looked at, as you mentioned before, the Jason on the website. How can we do better with this? So Jason is useful because it's human readable, which is helpful in the movie. But as you said, it is so. So we improved this by using protocol as our protocol. So overall, we managed to greatly improve the processing of this application by introducing cashing, harmonic scheduling, I'm reducing the pain. Okay, so now that we have talked about the processing of all this data, now let's look at how we display this data, 2 tradies. So we can't rely on automated changing strategies for everything, and human traders are really important part of the whole process. They manage, or they monitor the automated training strategies, and they're constantly treating strategies. There are also scenarios when we can't rely on the automation trains and strategies at all. For example, when there's a big news an accident or something unexpected happens. This can cause huge market moves, but it's not something that we can clearly define in code. Therefore, we rely on the trading to manually and react to this information. So we need to pick close attention to making sure that we display the data that matches most so that the trader can make the best trading decisions. So I'm sure many of you have taken ded data visualisation models in college. You may have been tasked for creating a data visualisation to display some data. You may have even created the visualisation of market data, such as displaying stock price movements over time. Now, 1st glance, this can be kind of a trivial task, but when you start to consider it more deeply, you may realise that there are more nuances here. We might need to consider the data granularity. Are you just saying data on a databases, on our new basis, or is another 2nd precision required? Do you need more information, like the vision they ask, the spread? Do you need to portray size at a given price point? But this challenge feels significantly when you're working in a continent trading frame, such as Susquehanna. How can we display the data, the market data, the pricing information, and everything that the trader needs to make a good decision, about 1000s of products, and favour it. In South Carolina, we deal with complex financial derivatives. A financial derivative is a contractor's value is provide for the price, or performance, of the one you like posit, such as a stocked body for inex. Does that even know of any financial germinus? Like a small position? Yeah, exactly. Yep. definitely. So, three examples are here, these options, features, and FX box, as you mentioned. So the largest business function path of Gahana is option trading. So because of you not familiar, an option is a contract, it is a holder device, but not the obligation to buy or sell an underlying acid, and some fixed price onward before a specified date in the future. That big price is called strike price. And the future date is the expiration date. So, for every possible stock, there are infinite combinations of options, contractors that can be created for every possible strike and expiration. Now, in reality, we focus on the subset of these, irregular intervals, abstrikes around you and your mind. So, now it will be market. Our original challenge, but our only issue we need to just play the relevant information for the stop, but also for all options on the stop. These are the options they imply volatility and say, that's a great option. So my voluntary is one important, most important variable to use spread pricing and option. So, this is a 3D graph of bipolatility, puddled against a strike on expiration. So, I think there are infinitely more data points here that we need to consider comprising an option, and there are good stuff. But I change don't just train options on single underlying. Instead, they're trading options on 100s of underlying on 100s of underlying. So the big task from Zoskehana is, how can we build systems that process massive bodies of data in your time? But also display these massive bodies to human traders in such a way that it's informative and digestible. So we avoid different techniques in Susquehanna. So often we actually display data numerically, not demographically. This is because numerical precision is easy to see when you're looking at an actual number rather than a graph. So it's a viewpoint, which is very trading, even 100 on a difference in the number, it can make a huge difference in their training decision. We also make use of colour to portray information to retreat it. For example, if we're showing their folks, we might shave those folks and their values to show it and they're talking about or not. We often use the sound lists. This is a technique I'm looking for you to go beyond the 2B view, but there's what's on the screen. So bring their attention to different market events and notifications. Nested views. So we're just lay the most important information on the top level. And then the training we need to be deeper if their attention was drawn to something in particular. And then also focussing, so there's a macro event, in particular, stop, the information for that stop will appear on the screen for the trainer to draw their attention to it quickly. So to read health, we've talked about the challenges of high performance data, granularity and optimisation, and displaying the data that matches most. So I hope this is giving you an insight into some of the work we do in Tosquehanna. And some of the ways that we and the challenges that we face when it comes to big data. So we see the very best in the rest of the Hakita. Myself and my colleagues will be around for the rest of the weekend, so we do encourage you to call the chat to us if you have any questions, right? like to learn more about this kind.